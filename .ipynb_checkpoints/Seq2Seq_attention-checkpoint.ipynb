{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of training examples: 29000\n",
      "Number of validation examples: 1014\n",
      "Number of testing examples: 1000\n",
      "{'src': ['zwei', 'junge', 'weiße', 'männer', 'sind', 'im', 'freien', 'in', 'der', 'nähe', 'vieler', 'büsche', '.'], 'trg': ['two', 'young', ',', 'white', 'males', 'are', 'outside', 'near', 'many', 'bushes', '.']}\n",
      "Unique tokens in source (de) vocabulary: 7853\n",
      "Unique tokens in target (en) vocabulary: 5893\n"
     ]
    }
   ],
   "source": [
    "# https://github.com/bentrevett/pytorch-seq2seq/blob/master/3%20-%20Neural%20Machine%20Translation%20by%20Jointly%20Learning%20to%20Align%20and%20Translate.ipynb\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "from torchtext.datasets import TranslationDataset, Multi30k\n",
    "from torchtext.data import Field, BucketIterator\n",
    "\n",
    "import spacy\n",
    "\n",
    "import random\n",
    "import math\n",
    "import os\n",
    "\n",
    "\n",
    "SEED = 1\n",
    "\n",
    "random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "\n",
    "spacy_de = spacy.load('de')\n",
    "spacy_en = spacy.load('en')\n",
    "\n",
    "def tokenize_de(text):\n",
    "    \"\"\"\n",
    "    Tokenizes German text from a string into a list of strings (tokens) and reverses it\n",
    "    \"\"\"\n",
    "    return [tok.text for tok in spacy_de.tokenizer(text)]\n",
    "\n",
    "def tokenize_en(text):\n",
    "    \"\"\"\n",
    "    Tokenizes English text from a string into a list of strings (tokens)\n",
    "    \"\"\"\n",
    "    return [tok.text for tok in spacy_en.tokenizer(text)]\n",
    "\n",
    "SRC = Field(tokenize=tokenize_de, init_token='<sos>', eos_token='<eos>', lower=True)\n",
    "TRG = Field(tokenize=tokenize_en, init_token='<sos>', eos_token='<eos>', lower=True)\n",
    "\n",
    "train_data, valid_data, test_data = Multi30k.splits(exts=('.de', '.en'), fields=(SRC, TRG))\n",
    "\n",
    "print(f\"Number of training examples: {len(train_data.examples)}\")\n",
    "print(f\"Number of validation examples: {len(valid_data.examples)}\")\n",
    "print(f\"Number of testing examples: {len(test_data.examples)}\")\n",
    "\n",
    "\n",
    "print(vars(train_data.examples[0]))\n",
    "\n",
    "SRC.build_vocab(train_data, min_freq=2)\n",
    "TRG.build_vocab(train_data, min_freq=2)\n",
    "\n",
    "print(f\"Unique tokens in source (de) vocabulary: {len(SRC.vocab)}\")\n",
    "print(f\"Unique tokens in target (en) vocabulary: {len(TRG.vocab)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n"
     ]
    }
   ],
   "source": [
    "BATCH_SIZE = 1\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(device)\n",
    "train_iterator, valid_iterator, test_iterator = BucketIterator.splits(\n",
    "    (train_data, valid_data, test_data), batch_size=BATCH_SIZE, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(self, input_dim, emb_dim, enc_hid_dim, dec_hid_dim, dropout):\n",
    "        super().__init__()\n",
    "        self.input_dim = input_dim\n",
    "        self.emb_dim = emb_dim\n",
    "        self.enc_hid_dim = enc_hid_dim\n",
    "        self.dec_hid_dim = dec_hid_dim\n",
    "        self.dropout = dropout\n",
    "        \n",
    "        self.embedding = nn.Embedding(input_dim, emb_dim)\n",
    "        self.rnn = nn.GRU(emb_dim, enc_hid_dim, bidirectional=True)\n",
    "        \n",
    "        self.linear = nn.Linear(enc_hid_dim*2, dec_hid_dim)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "        \n",
    "    def forward(self, input):\n",
    "        \n",
    "        # input dim => (src_sent_len, batch_size)\n",
    "        embed = self.dropout(self.embedding(input))\n",
    "        \n",
    "        # embed dim => (src_sent_len, batch_size, emb_dim)\n",
    "        outputs, hidden = self.rnn(embed)\n",
    "        \n",
    "        # outputs dim => (src_sent_len, batch size, enc_hid_dim * num directions)\n",
    "        # hidden dim => (number of layers * number of directions, batch size, enc_hid_dim)\n",
    "        \n",
    "        hidden = torch.tanh(self.linear(torch.cat((hidden[-2, : , :], hidden[-1, :, :]), dim = 1)))\n",
    "        \n",
    "        # hidden[-2, :, :] reduces 3d to 2d tensor since first dimension is now fixed, so dim = 1 is the last dimension\n",
    "        # hidden dim => (batch size, dec_hid_dim)\n",
    "        \n",
    "        return outputs, hidden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Attention(nn.Module):\n",
    "    def __init__(self, enc_hid_dim, dec_hid_dim):\n",
    "        super().__init__()\n",
    "        self.enc_hid_dim = enc_hid_dim\n",
    "        self.dec_hid_dim = dec_hid_dim\n",
    "        \n",
    "        self.attn = nn.Linear(enc_hid_dim * 2 + dec_hid_dim, dec_hid_dim)\n",
    "        self.v = nn.Parameter(torch.randn(dec_hid_dim))\n",
    "        \n",
    "    def forward(self, hidden, encoder_outputs):\n",
    "        \n",
    "        # hidden should be => (number of layers * number of directions, batch size, dec_hid_dim))\n",
    "        # but the author did a squeeze operation in the decoder before returning the last hidden state\n",
    "        # so hidden dimension becomes => (batch size, dec_hid_dim)\n",
    "        # another reason to do this would be keep hidden dim similar in both encoder and decoder\n",
    "        \n",
    "        # hidden dimension becomes => (batch size, dec_hid_dim)\n",
    "        # encoder_outputs dimension => (src_sent_len, batch size, enc_hid_dim * num directions)\n",
    "        \n",
    "        \n",
    "        batch_size = encoder_outputs.shape[1]\n",
    "        src_len = encoder_outputs.shape[0]\n",
    "        \n",
    "        hidden = hidden.unsqueeze(1).repeat(1, src_len,1)\n",
    "        # hidden new dim = > (batch_size, src_len, dec_hid_dim) \n",
    "        encoder_outputs = encoder_outputs.permute(1, 0, 2)\n",
    "        # encoder_outputs new dim = > (batch_size, src_len, enc_hid_dim * 2) \n",
    "        energy = torch.tanh(self.attn(torch.cat((hidden, encoder_outputs), dim = 2)))\n",
    "        \n",
    "        # attn dim = > (batch_size, src_len, dec_hid_dim)\n",
    "        energy = energy.permute(0, 2, 1)\n",
    "        \n",
    "        # we want to compute energy whose dimension is => (batch size, dec_hid_dim, source sent len)\n",
    "        \n",
    "        # v dim should be => (batch_size, 1, dec-hid_dim)\n",
    "        v = self.v.repeat(batch_size, 1).unsqueeze(1)\n",
    "        \n",
    "        # attention dim (batch_size, 1, src_len)\n",
    "        # torch.bmm(batch1, batch2, out=None) → Tensor\n",
    "        # If batch1 is a (b×n×m) tensor, batch2 is a (b×m×p) tensor, out will be a (b×n×p) tensor\n",
    "        attention = torch.bmm(v, energy).squeeze(1)\n",
    "        \n",
    "        # attention dim (batch_size, src_len)\n",
    "        return F.softmax(attention, dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):\n",
    "    def __init__(self, output_dim, emb_dim, enc_hid_dim, dec_hid_dim, dropout, attention):\n",
    "        super().__init__()\n",
    "        self.output_dim = output_dim\n",
    "        self.emb_dim = emb_dim\n",
    "        self.enc_hid_dim = enc_hid_dim\n",
    "        self.dec_hid_dim = dec_hid_dim\n",
    "        self.dropout = dropout\n",
    "        self.attention = attention\n",
    "        \n",
    "        self.embedding = nn.Embedding(output_dim, emb_dim)\n",
    "        self.rnn = nn.GRU(enc_hid_dim * 2 + emb_dim, dec_hid_dim)\n",
    "        self.fc = nn.Linear(enc_hid_dim * 2 + emb_dim + dec_hid_dim, output_dim)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "    def forward(self, input, encoder_outputs, hidden):\n",
    "        \n",
    "        # input dim => [batch_size]\n",
    "        # encoder_outputs = [src sent len, batch size, enc hid dim * 2]\n",
    "        print(\"decoder input 1\")\n",
    "        print(input)\n",
    "        input = input.unsqueeze(0)\n",
    "        # input dim => [1, batch_size]\n",
    "        print(\"decoder input 2\")\n",
    "        print(input)\n",
    "\n",
    "\n",
    "        embed = self.dropout(self.embedding(input))\n",
    "        print(\"decoder input 3\")\n",
    "        \n",
    "\n",
    "        # embed dimension => (1, batch_size, emb_dim)\n",
    "        \n",
    "        a = self.attention(hidden, encoder_outputs)\n",
    "        # a dim => (batch_size, src_len)\n",
    "        #encoder_outputs = [src sent len, batch size, enc hid dim * 2]\n",
    "        a = a.unsqueeze(1)\n",
    "        # a dim => (batch_size, 1, src_len)\n",
    "        encoder_outputs = encoder_outputs.permute(1, 0, 2)\n",
    "        # a dim => (batch_size, 1, src_len)\n",
    "        #encoder_outputs = [batch size, src sent len, enc hid dim * 2]\n",
    "        \n",
    "        weighted = torch.bmm(a, encoder_outputs)\n",
    "        # weighted dim => (batch_size, 1, enc_hid_dim * 2)\n",
    "        weighted = weighted.permute(1, 0, 2)\n",
    "        # weighted dim => (1, batch_size, enc_hid_dim * 2)\n",
    "\n",
    "        rnn_input = torch.cat((weighted, embed), dim=2)\n",
    "        #hidden = [batch size, dec hid dim]\n",
    "        output, hidden = self.rnn(rnn_input, hidden.unsqueeze(0))\n",
    "        \n",
    "        # outputs dim => (src_sent_len, batch size, enc_hid_dim * num directions)\n",
    "        # hidden dim => (number of layers * number of directions, batch size, enc_hid_dim)\n",
    "        \n",
    "        # here src_sent_len = number of layers = number of directions = 1 for decoder only\n",
    "        # sp basically\n",
    "        # outputs dim => (1, batch size, enc_hid_dim * num directions)\n",
    "        # hidden dim => (1, batch size, enc_hid_dim)\n",
    "        \n",
    "        \n",
    "        prediction = self.fc(torch.cat((output.squeeze(0), weighted.squeeze(0), embed.squeeze(0)), dim=1))\n",
    "        return prediction, hidden.squeeze(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Seq2Seq(nn.Module):\n",
    "    def __init__(self, encoder, decoder, device):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "        self.device = device\n",
    "        \n",
    "        \n",
    "    def forward(self, src, trg, teacher_forcing_ratio=0.5):\n",
    "        \n",
    "        #src = [sent len, batch size]\n",
    "        #trg = [sent len, batch size]\n",
    "        #teacher_forcing_ratio is probability to use teacher forcing\n",
    "        #e.g. if teacher_forcing_ratio is 0.75 we use ground-truth inputs 75% of the time\n",
    "        \n",
    "        batch_size = trg.shape[1]\n",
    "        max_len = trg.shape[0]\n",
    "        print(\"trg shape\", trg.shape)\n",
    "        trg_vocab_size = self.decoder.output_dim\n",
    "        print(\"batch size \", batch_size)\n",
    "        print(\"max len \", max_len)\n",
    "        print(\"trg vocab size \", trg_vocab_size)\n",
    "        \n",
    "        #tensor to store decoder outputs\n",
    "        outputs = torch.zeros(max_len, batch_size, trg_vocab_size).to(self.device)\n",
    "        print(\"shape of outputs\")\n",
    "        print(outputs.shape)\n",
    "        #last hidden state of the encoder is used as the initial hidden state of the decoder\n",
    "        encoder_outputs, hidden = self.encoder(src)\n",
    "        #first input to the decoder is the <sos> tokens\n",
    "        output = trg[0,:]\n",
    "        print(\"first input\")\n",
    "        print(output)\n",
    "        for t in range(1, max_len):\n",
    "            print(\"in LOOP\")\n",
    "            output, hidden = self.decoder(output, encoder_outputs, hidden)\n",
    "            outputs[t] = output\n",
    "            teacher_force = random.random() < teacher_forcing_ratio\n",
    "            print(\"output in loop\")\n",
    "            print(output)\n",
    "            print(output.max(1))\n",
    "            top1 = output.max(1)[1] # index of the max value\n",
    "            output = (trg[t] if teacher_force else top1)\n",
    "        \n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "INPUT_DIM = len(SRC.vocab)\n",
    "OUTPUT_DIM = len(TRG.vocab)\n",
    "ENC_EMB_DIM = 256\n",
    "DEC_EMB_DIM = 256\n",
    "ENC_HID_DIM = 512\n",
    "DEC_HID_DIM = 512\n",
    "ENC_DROPOUT = 0.5\n",
    "DEC_DROPOUT = 0.5\n",
    "\n",
    "attn = Attention(ENC_HID_DIM, DEC_HID_DIM)\n",
    "enc = Encoder(INPUT_DIM, ENC_EMB_DIM, ENC_HID_DIM, DEC_HID_DIM, ENC_DROPOUT)\n",
    "dec = Decoder(OUTPUT_DIM, DEC_EMB_DIM, ENC_HID_DIM, DEC_HID_DIM, DEC_DROPOUT, attn)\n",
    "\n",
    "model = Seq2Seq(enc, dec, device).to(device)\n",
    "\n",
    "optimizer = optim.Adam(model.parameters())\n",
    "\n",
    "pad_idx = TRG.vocab.stoi['<pad>']\n",
    "\n",
    "criterion = nn.CrossEntropyLoss(ignore_index=pad_idx)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, iterator, optimizer, criterion, clip):\n",
    "    \n",
    "    model.train()\n",
    "    \n",
    "    epoch_loss = 0\n",
    "    \n",
    "    for i, batch in enumerate(iterator):\n",
    "        \n",
    "        src = batch.src\n",
    "        trg = batch.trg\n",
    "        print(\"src\")\n",
    "        print(src)\n",
    "        print(\"target\")\n",
    "        print(trg)\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        output = model(src, trg)\n",
    "        print(\"output after seq2seq\")\n",
    "        print(output)\n",
    "        print(output.shape)\n",
    "        #trg = [sent len, batch size]\n",
    "        #output = [sent len, batch size, output dim]\n",
    "        \n",
    "        #reshape to:\n",
    "        #trg = [(sent len - 1) * batch size]\n",
    "        #output = [(sent len - 1) * batch size, output dim]\n",
    "        x = output[1:].view(-1, output.shape[2])\n",
    "        y = trg[1:].view(-1)\n",
    "        print(\"X \", x.shape)\n",
    "        print(x[0])\n",
    "        print(\"Y \", y.shape)\n",
    "        print(y)\n",
    "        loss = criterion(output[1:].view(-1, output.shape[2]), trg[1:].view(-1))\n",
    "        \n",
    "        loss.backward()\n",
    "        \n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), clip)\n",
    "        \n",
    "        optimizer.step()\n",
    "        \n",
    "        epoch_loss += loss.item()\n",
    "        break\n",
    "        \n",
    "    return epoch_loss / len(iterator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, iterator, criterion):\n",
    "    \n",
    "    model.eval()\n",
    "    \n",
    "    epoch_loss = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "    \n",
    "        for i, batch in enumerate(iterator):\n",
    "\n",
    "            src = batch.src\n",
    "            trg = batch.trg\n",
    "\n",
    "            output = model(src, trg, 0) #turn off teacher forcing\n",
    "\n",
    "            loss = criterion(output[1:].view(-1, output.shape[2]), trg[1:].view(-1))\n",
    "\n",
    "            epoch_loss += loss.item()\n",
    "        \n",
    "    return epoch_loss / len(iterator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "src\n",
      "tensor([[   2],\n",
      "        [  45],\n",
      "        [   7],\n",
      "        [1384],\n",
      "        [   9],\n",
      "        [   0],\n",
      "        [  97],\n",
      "        [ 185],\n",
      "        [  23],\n",
      "        [  12],\n",
      "        [   6],\n",
      "        [ 338],\n",
      "        [  47],\n",
      "        [   6],\n",
      "        [5738],\n",
      "        [   4],\n",
      "        [   3]])\n",
      "target\n",
      "tensor([[   2],\n",
      "        [  50],\n",
      "        [   6],\n",
      "        [ 263],\n",
      "        [ 990],\n",
      "        [ 219],\n",
      "        [ 688],\n",
      "        [   6],\n",
      "        [   4],\n",
      "        [ 275],\n",
      "        [  71],\n",
      "        [  18],\n",
      "        [   4],\n",
      "        [1075],\n",
      "        [ 100],\n",
      "        [   5],\n",
      "        [   3]])\n",
      "trg shape torch.Size([17, 1])\n",
      "batch size  1\n",
      "max len  17\n",
      "trg vocab size  5893\n",
      "shape of outputs\n",
      "torch.Size([17, 1, 5893])\n",
      "first input\n",
      "tensor([2])\n",
      "in LOOP\n",
      "decoder input 1\n",
      "tensor([2])\n",
      "decoder input 2\n",
      "tensor([[2]])\n",
      "decoder input 3\n",
      "output in loop\n",
      "tensor([[ 0.8845,  0.0151, -0.3956,  ...,  0.0014,  0.1506,  0.9206]],\n",
      "       grad_fn=<ThAddmmBackward>)\n",
      "(tensor([1.2417], grad_fn=<MaxBackward0>), tensor([665]))\n",
      "in LOOP\n",
      "decoder input 1\n",
      "tensor([50])\n",
      "decoder input 2\n",
      "tensor([[50]])\n",
      "decoder input 3\n",
      "output in loop\n",
      "tensor([[ 2.1445e-01, -2.4509e-01, -4.1073e-01,  ...,  1.0019e-01,\n",
      "          2.1196e-01, -2.1619e-02]], grad_fn=<ThAddmmBackward>)\n",
      "(tensor([1.2476], grad_fn=<MaxBackward0>), tensor([428]))\n",
      "in LOOP\n",
      "decoder input 1\n",
      "tensor([428])\n",
      "decoder input 2\n",
      "tensor([[428]])\n",
      "decoder input 3\n",
      "output in loop\n",
      "tensor([[ 0.3274, -0.6515, -0.0497,  ..., -0.3166,  0.8332,  0.5335]],\n",
      "       grad_fn=<ThAddmmBackward>)\n",
      "(tensor([1.3523], grad_fn=<MaxBackward0>), tensor([2465]))\n",
      "in LOOP\n",
      "decoder input 1\n",
      "tensor([2465])\n",
      "decoder input 2\n",
      "tensor([[2465]])\n",
      "decoder input 3\n",
      "output in loop\n",
      "tensor([[ 0.7873, -0.2045,  0.4979,  ...,  0.1698,  0.0734,  0.8349]],\n",
      "       grad_fn=<ThAddmmBackward>)\n",
      "(tensor([1.2606], grad_fn=<MaxBackward0>), tensor([371]))\n",
      "in LOOP\n",
      "decoder input 1\n",
      "tensor([990])\n",
      "decoder input 2\n",
      "tensor([[990]])\n",
      "decoder input 3\n",
      "output in loop\n",
      "tensor([[ 0.1981, -0.1932, -0.0981,  ...,  0.2908, -0.1637,  0.3845]],\n",
      "       grad_fn=<ThAddmmBackward>)\n",
      "(tensor([1.3799], grad_fn=<MaxBackward0>), tensor([3463]))\n",
      "in LOOP\n",
      "decoder input 1\n",
      "tensor([219])\n",
      "decoder input 2\n",
      "tensor([[219]])\n",
      "decoder input 3\n",
      "output in loop\n",
      "tensor([[-2.5312e-01, -2.4986e-01, -3.0111e-02,  ..., -2.2819e-01,\n",
      "          1.0004e-01,  7.0212e-02]], grad_fn=<ThAddmmBackward>)\n",
      "(tensor([1.2663], grad_fn=<MaxBackward0>), tensor([3070]))\n",
      "in LOOP\n",
      "decoder input 1\n",
      "tensor([688])\n",
      "decoder input 2\n",
      "tensor([[688]])\n",
      "decoder input 3\n",
      "output in loop\n",
      "tensor([[ 1.0449, -0.6764, -0.5220,  ...,  0.5793,  0.3484, -0.0975]],\n",
      "       grad_fn=<ThAddmmBackward>)\n",
      "(tensor([1.4302], grad_fn=<MaxBackward0>), tensor([3975]))\n",
      "in LOOP\n",
      "decoder input 1\n",
      "tensor([3975])\n",
      "decoder input 2\n",
      "tensor([[3975]])\n",
      "decoder input 3\n",
      "output in loop\n",
      "tensor([[ 0.2216, -0.1926, -0.2656,  ...,  0.0852,  0.0299,  0.2461]],\n",
      "       grad_fn=<ThAddmmBackward>)\n",
      "(tensor([1.1590], grad_fn=<MaxBackward0>), tensor([4749]))\n",
      "in LOOP\n",
      "decoder input 1\n",
      "tensor([4749])\n",
      "decoder input 2\n",
      "tensor([[4749]])\n",
      "decoder input 3\n",
      "output in loop\n",
      "tensor([[ 0.2360, -0.2051,  0.4525,  ..., -0.3026, -0.4457, -0.1170]],\n",
      "       grad_fn=<ThAddmmBackward>)\n",
      "(tensor([1.2691], grad_fn=<MaxBackward0>), tensor([4068]))\n",
      "in LOOP\n",
      "decoder input 1\n",
      "tensor([275])\n",
      "decoder input 2\n",
      "tensor([[275]])\n",
      "decoder input 3\n",
      "output in loop\n",
      "tensor([[ 0.2479, -0.2392, -0.0634,  ..., -0.4236,  0.2918, -0.1586]],\n",
      "       grad_fn=<ThAddmmBackward>)\n",
      "(tensor([1.1340], grad_fn=<MaxBackward0>), tensor([2327]))\n",
      "in LOOP\n",
      "decoder input 1\n",
      "tensor([71])\n",
      "decoder input 2\n",
      "tensor([[71]])\n",
      "decoder input 3\n",
      "output in loop\n",
      "tensor([[ 0.0523, -0.5505,  0.0522,  ...,  0.0801,  0.3715, -0.1596]],\n",
      "       grad_fn=<ThAddmmBackward>)\n",
      "(tensor([1.2021], grad_fn=<MaxBackward0>), tensor([4989]))\n",
      "in LOOP\n",
      "decoder input 1\n",
      "tensor([4989])\n",
      "decoder input 2\n",
      "tensor([[4989]])\n",
      "decoder input 3\n",
      "output in loop\n",
      "tensor([[ 0.3475, -0.6263, -0.0286,  ...,  0.4060,  0.2890, -0.0315]],\n",
      "       grad_fn=<ThAddmmBackward>)\n",
      "(tensor([1.4450], grad_fn=<MaxBackward0>), tensor([4665]))\n",
      "in LOOP\n",
      "decoder input 1\n",
      "tensor([4])\n",
      "decoder input 2\n",
      "tensor([[4]])\n",
      "decoder input 3\n",
      "output in loop\n",
      "tensor([[ 4.5420e-01, -5.2453e-01, -3.3469e-04,  ...,  4.3197e-01,\n",
      "          3.0085e-01,  2.1209e-01]], grad_fn=<ThAddmmBackward>)\n",
      "(tensor([1.2061], grad_fn=<MaxBackward0>), tensor([3278]))\n",
      "in LOOP\n",
      "decoder input 1\n",
      "tensor([3278])\n",
      "decoder input 2\n",
      "tensor([[3278]])\n",
      "decoder input 3\n",
      "output in loop\n",
      "tensor([[ 9.3198e-02, -3.8589e-01, -6.6611e-01,  ..., -1.4026e-01,\n",
      "          1.2809e-01,  2.1687e-01]], grad_fn=<ThAddmmBackward>)\n",
      "(tensor([1.1550], grad_fn=<MaxBackward0>), tensor([2787]))\n",
      "in LOOP\n",
      "decoder input 1\n",
      "tensor([100])\n",
      "decoder input 2\n",
      "tensor([[100]])\n",
      "decoder input 3\n",
      "output in loop\n",
      "tensor([[-3.0036e-03, -4.0973e-01,  9.0775e-02,  ...,  2.9723e-01,\n",
      "         -1.8220e-01, -3.6464e-01]], grad_fn=<ThAddmmBackward>)\n",
      "(tensor([1.2076], grad_fn=<MaxBackward0>), tensor([3795]))\n",
      "in LOOP\n",
      "decoder input 1\n",
      "tensor([5])\n",
      "decoder input 2\n",
      "tensor([[5]])\n",
      "decoder input 3\n",
      "output in loop\n",
      "tensor([[ 0.6431, -0.8265, -0.2079,  ...,  0.4588, -0.0580, -0.3962]],\n",
      "       grad_fn=<ThAddmmBackward>)\n",
      "(tensor([1.2418], grad_fn=<MaxBackward0>), tensor([574]))\n",
      "output after seq2seq\n",
      "tensor([[[ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000]],\n",
      "\n",
      "        [[ 0.8845,  0.0151, -0.3956,  ...,  0.0014,  0.1506,  0.9206]],\n",
      "\n",
      "        [[ 0.2145, -0.2451, -0.4107,  ...,  0.1002,  0.2120, -0.0216]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[ 0.0932, -0.3859, -0.6661,  ..., -0.1403,  0.1281,  0.2169]],\n",
      "\n",
      "        [[-0.0030, -0.4097,  0.0908,  ...,  0.2972, -0.1822, -0.3646]],\n",
      "\n",
      "        [[ 0.6431, -0.8265, -0.2079,  ...,  0.4588, -0.0580, -0.3962]]],\n",
      "       grad_fn=<CopySlices>)\n",
      "torch.Size([17, 1, 5893])\n",
      "X  torch.Size([16, 5893])\n",
      "tensor([ 0.8845,  0.0151, -0.3956,  ...,  0.0014,  0.1506,  0.9206],\n",
      "       grad_fn=<SelectBackward>)\n",
      "Y  torch.Size([16])\n",
      "tensor([  50,    6,  263,  990,  219,  688,    6,    4,  275,   71,   18,    4,\n",
      "        1075,  100,    5,    3])\n",
      "| Epoch: 001 | Train Loss: 0.000 | Train PPL:   1.000  |\n"
     ]
    }
   ],
   "source": [
    "N_EPOCHS = 1\n",
    "CLIP = 10\n",
    "SAVE_DIR = 'models'\n",
    "MODEL_SAVE_PATH = os.path.join(SAVE_DIR, 'tut1_model.pt')\n",
    "\n",
    "best_valid_loss = float('inf')\n",
    "\n",
    "if not os.path.isdir(f'{SAVE_DIR}'):\n",
    "    os.makedirs(f'{SAVE_DIR}')\n",
    "\n",
    "for epoch in range(N_EPOCHS):\n",
    "    \n",
    "    train_loss = train(model, train_iterator, optimizer, criterion, CLIP)\n",
    "#     valid_loss = evaluate(model, valid_iterator, criterion)\n",
    "    \n",
    "#     if valid_loss < best_valid_loss:\n",
    "#         best_valid_loss = valid_loss\n",
    "#         torch.save(model.state_dict(), MODEL_SAVE_PATH)\n",
    "    \n",
    "#     print(f'| Epoch: {epoch+1:03} | Train Loss: {train_loss:.3f} | Train PPL: {math.exp(train_loss):7.3f} | Val. Loss: {valid_loss:.3f} | Val. PPL: {math.exp(valid_loss):7.3f} |')\n",
    "    print(f'| Epoch: {epoch+1:03} | Train Loss: {train_loss:.3f} | Train PPL: {math.exp(train_loss):7.3f}  |')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load_state_dict(torch.load(MODEL_SAVE_PATH))\n",
    "\n",
    "test_loss = evaluate(model, test_iterator, criterion)\n",
    "\n",
    "print(f'| Test Loss: {test_loss:.3f} | Test PPL: {math.exp(test_loss):7.3f} |')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
